import streamlit as st
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from imblearn.over_sampling import SMOTE
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.neighbors import KNeighborsClassifier
from sklearn.svm import SVC
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier
from xgboost import XGBClassifier
from lightgbm import LGBMClassifier
from sklearn.neural_network import MLPClassifier
from sklearn.metrics import classification_report, roc_auc_score, f1_score, confusion_matrix, roc_curve, ConfusionMatrixDisplay
import shap
from sklearn.cluster import KMeans, DBSCAN
from sklearn.decomposition import PCA
from sklearn.metrics import silhouette_score
from scipy.spatial.distance import cdist # Para o m√©todo do cotovelo (dist√¢ncia)
from sklearn.neighbors import NearestNeighbors # Para o gr√°fico k-dist√¢ncia

# Configura√ß√µes iniciais do Streamlit
st.set_page_config(layout="wide", page_title="An√°lise de Risco de Cr√©dito - Expert Gem")

st.title("Sistema de Apoio √† Decis√£o: An√°lise de Risco de Cr√©dito üí≥")
st.markdown("Bem-vindo ao seu dashboard interativo, desenvolvido por seu expert em Python e An√°lise de Dados!")

# --- Se√ß√£o de Upload e Carregamento de Dados ---
st.header("1. Carregamento e Diagn√≥stico dos Dados")
st.write("Fa√ßa o upload do arquivo `credit_customers.csv` ou use o dataset de exemplo.")

uploaded_file = st.file_uploader("Escolha um arquivo CSV", type="csv")

try:
    df = pd.read_csv("credit_customers.csv")
    st.success("Dataset `credit_customers.csv` carregado com sucesso!")
except FileNotFoundError:
    st.error("Erro: O arquivo `credit_customers.csv` n√£o foi encontrado no reposit√≥rio. Por favor, certifique-se de que ele esteja na raiz do seu Hugging Face Space.")
    st.stop()


if 'df' in locals(): # Garante que o dataframe foi carregado
    st.subheader("Pr√©via dos Dados")
    st.dataframe(df.head())
    st.write(f"Total de {len(df)} registros e {len(df.columns)} colunas.")

    st.subheader("Distribui√ß√£o da Vari√°vel Alvo (`class`)")
    fig1, ax1 = plt.subplots(figsize=(8, 5))
    sns.countplot(data=df, x='class', ax=ax1, palette='viridis')
    ax1.set_title('Distribui√ß√£o de Bons e Maus Pagadores')
    ax1.set_xlabel('Classe (good = pagador, bad = inadimplente)')
    ax1.set_ylabel('Contagem')
    st.pyplot(fig1)

    class_counts = df['class'].value_counts(normalize=True) * 100
    st.write("Porcentagem da Classe:")
    st.write(class_counts)
    st.markdown(f"**Diagn√≥stico:** A vari√°vel alvo 'class' est√° desbalanceada, com **{class_counts.loc['good']:.2f}% de 'good' (bons pagadores)** e **{class_counts.loc['bad']:.2f}% de 'bad' (maus pagadores)**. Isso pode enviesar os modelos e ser√° tratado com SMOTE.")

    # --- Pr√©-processamento e SMOTE ---
    st.header("2. Pr√©-processamento e Balanceamento com SMOTE")

    # Codifica a vari√°vel alvo
    df['class'] = df['class'].map({'good': 0, 'bad': 1})
    X = df.drop('class', axis=1)
    y = df['class']

    # Identificar colunas categ√≥ricas e num√©ricas
    categorical_features = X.select_dtypes(include=['object']).columns
    numeric_features = X.select_dtypes(include=['float64', 'int64']).columns

    # Criar pr√©-processador
    preprocessor = ColumnTransformer(
        transformers=[
            ('num', StandardScaler(), numeric_features),
            ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_features)
        ])

    st.write("Aplicando pr√©-processamento (Padroniza√ß√£o para Num√©ricas, One-Hot Encoding para Categ√≥ricas)...")
    X_processed = preprocessor.fit_transform(X)

    st.write("Aplicando SMOTE para balancear as classes...")
    smote = SMOTE(random_state=42)
    X_res, y_res = smote.fit_resample(X_processed, y)

    st.subheader("Distribui√ß√£o das Classes ap√≥s SMOTE")
    fig2, ax2 = plt.subplots(figsize=(8, 5))
    sns.countplot(x=y_res, ax=ax2, palette='viridis')
    ax2.set_title('Distribui√ß√£o das Classes ap√≥s SMOTE')
    ax2.set_xlabel('Classe (0 = good, 1 = bad)')
    ax2.set_ylabel('Contagem')
    st.pyplot(fig2)

    unique, counts = np.unique(y_res, return_counts=True)
    st.write("Nova Propor√ß√£o de Classes ap√≥s SMOTE:")
    st.write(dict(zip(unique, counts)))
   st.markdown("Com o SMOTE, as classes 'good' (0) e 'bad' (1) est√£o agora **balanceadas com 700 registros cada**, o que √© ideal para o treinamento dos modelos e evita vi√©s para a classe majorit√°ria.")
    # --- An√°lise Preditiva com Modelos Supervisionados ---
    st.header("3. An√°lise Preditiva com Modelos Supervisionados")
    st.write("Dividindo os dados em conjuntos de treino e teste (70/30)...")
    X_train, X_test, y_train, y_test = train_test_split(X_res, y_res, test_size=0.3, random_state=42)

    modelos = {
        "KNN": KNeighborsClassifier(),
        "SVM": SVC(probability=True, random_state=42),
        "Decision Tree": DecisionTreeClassifier(random_state=42),
        "Random Forest": RandomForestClassifier(random_state=42),
        "AdaBoost": AdaBoostClassifier(random_state=42),
        "Gradient Boosting": GradientBoostingClassifier(random_state=42),
        "XGBoost": XGBClassifier(eval_metric='logloss', use_label_encoder=False, random_state=42),
        "LightGBM": LGBMClassifier(random_state=42),
        "MLP": MLPClassifier(max_iter=1000, random_state=42)
    }

    resultados = []
    roc_curves_data = []

    st.subheader("Treinamento e Avalia√ß√£o dos Modelos")
    progress_bar = st.progress(0)
    status_text = st.empty()

    for i, (nome, modelo) in enumerate(modelos.items()):
        status_text.text(f"Treinando e avaliando: {nome}...")
        modelo.fit(X_train, y_train)
        y_pred = modelo.predict(X_test)
        y_proba = modelo.predict_proba(X_test)[:, 1] # Probabilidade da classe 1 ('bad')

        auc = roc_auc_score(y_test, y_proba)
        f1 = f1_score(y_test, y_pred)
        report = classification_report(y_test, y_pred, output_dict=True)

        fpr, tpr, _ = roc_curve(y_test, y_proba)
        roc_curves_data.append({'model': nome, 'fpr': fpr, 'tpr': tpr})

        resultados.append({
            'Modelo': nome,
            'AUC': auc,
            'F1-score': f1,
            'Precision (Bad)': report['1']['precision'], # Precis√£o para a classe 'bad' (1)
            'Recall (Bad)': report['1']['recall']     # Recall para a classe 'bad' (1)
        })

        # Matriz de Confus√£o
        cm = confusion_matrix(y_test, y_pred, labels=modelo.classes_)
        disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['good (0)', 'bad (1)'])
        fig_cm, ax_cm = plt.subplots(figsize=(5, 5))
        disp.plot(cmap='Blues', ax=ax_cm)
        ax_cm.set_title(f'Matriz de Confus√£o: {nome}')
        st.pyplot(fig_cm)
        plt.close(fig_cm) # Fecha a figura para n√£o consumir mem√≥ria

        progress_bar.progress((i + 1) / len(modelos))

    df_resultados = pd.DataFrame(resultados).sort_values(by='AUC', ascending=False).reset_index(drop=True)
    status_text.success("Avalia√ß√£o dos modelos conclu√≠da!")

    st.subheader("Tabela de Resultados dos Modelos")
    st.dataframe(df_resultados)

    st.subheader("Curvas ROC dos Modelos")
    fig_roc, ax_roc = plt.subplots(figsize=(10, 7))
    for roc_data in roc_curves_data:
        ax_roc.plot(roc_data['fpr'], roc_data['tpr'], label=roc_data['model'])
    ax_roc.plot([0, 1], [0, 1], 'k--', label='Aleat√≥rio')
    ax_roc.set_title('Curvas ROC dos Modelos')
    ax_roc.set_xlabel('Taxa de Falsos Positivos')
    ax_roc.set_ylabel('Taxa de Verdadeiros Positivos')
    ax_roc.legend()
    ax_roc.grid(True)
    st.pyplot(fig_roc)
    plt.close(fig_roc) # Fecha a figura

    best_model_name = df_resultados.iloc[0]['Modelo']
    [cite_start]st.markdown(f"**Melhor Modelo Selecionado:** O **{best_model_name}** foi escolhido como o modelo de melhor desempenho geral, apresentando a maior pontua√ß√£o AUC ({df_resultados.iloc[0]['AUC']:.4f}). [cite: 482]")
    [cite_start]st.markdown("A AUC √© uma m√©trica robusta para problemas de classifica√ß√£o bin√°ria e √© particularmente √∫til para avaliar a capacidade do modelo de distinguir entre as classes, especialmente em cen√°rios de risco de cr√©dito onde tanto a taxa de verdadeiros positivos quanto a de falsos positivos s√£o importantes. [cite: 483]")
    [cite_start]st.markdown(f"O {best_model_name}, junto com MLP e LightGBM, demonstraram as curvas mais pr√≥ximas do canto superior esquerdo, indicando excelente capacidade de discrimina√ß√£o entre bons e maus pagadores. [cite: 485]")

    # --- Explicabilidade com SHAP ---
    st.header("4. Explicabilidade (XAI) com SHAP Values")
    st.write(f"Aplicando SHAP (SHapley Additive exPlanations) ao modelo {best_model_name} para entender as influ√™ncias das caracter√≠sticas nas previs√µes de risco.")

    modelo_escolhido = modelos[best_model_name]

    # Obter nomes das colunas ap√≥s OneHotEncoding
    cat_cols_original = X.select_dtypes(include='object').columns
    onehot_encoder = preprocessor.named_transformers_['cat']
    encoded_feature_names = onehot_encoder.get_feature_names_out(cat_cols_original)
    all_feature_names = list(numeric_features) + list(encoded_feature_names)

    # Converter X_test para DataFrame com nomes das colunas
    X_test_df = pd.DataFrame(X_test, columns=all_feature_names)

    # SHAP Explainer
    explainer = shap.TreeExplainer(modelo_escolhido) # TreeExplainer para modelos baseados em √°rvore
    shap_values = explainer.shap_values(X_test_df)
    
    # Se o modelo tem m√∫ltiplas sa√≠das (como classifica√ß√£o bin√°ria), shap_values √© uma lista. Pegamos a classe 'bad' (1)
    if isinstance(shap_values, list):
        shap_values_class_1 = shap_values[1] # Para a classe 'bad' (1)
    else:
        shap_values_class_1 = shap_values

    st.subheader("SHAP Summary Plot: Impacto Global das Caracter√≠sticas")
    [cite_start]st.write("Este gr√°fico visualiza a import√¢ncia geral e a dire√ß√£o do impacto de cada caracter√≠stica na previs√£o de inadimpl√™ncia ('bad'). [cite: 631]")
    st.write("Pontos vermelhos indicam valores altos da caracter√≠stica, pontos azuis indicam valores baixos. A posi√ß√£o horizontal mostra o impacto na previs√£o.")

    fig_shap_summary, ax_shap_summary = plt.subplots(figsize=(12, 8))
    shap.summary_plot(shap_values_class_1, X_test_df, plot_type="dot", max_display=20, show=False, ax=ax_shap_summary)
    fig_shap_summary.tight_layout()
    st.pyplot(fig_shap_summary)
    plt.close(fig_shap_summary)

    st.markdown(
        """
        **Interpreta√ß√£o:**
        * [cite_start]**`duration` (dura√ß√£o do cr√©dito):** √â a caracter√≠stica mais influente. Valores mais baixos de duration tendem a diminuir a probabilidade de inadimpl√™ncia (pontos azuis √† esquerda), enquanto dura√ß√µes mais longas (pontos vermelhos √† direita) aumentam a probabilidade de inadimpl√™ncia. [cite: 635]
        * **`credit_amount` (valor do cr√©dito):** A segunda caracter√≠stica mais importante. [cite_start]Valores de `credit_amount` mais baixos contribuem para a classifica√ß√£o de bom pagador, enquanto valores mais altos elevam o risco de ser um mau pagador. [cite: 636]
        * [cite_start]Outras caracter√≠sticas como `age` (idade), `checking_status` (status da conta corrente) e `purpose` (prop√≥sito do cr√©dito) tamb√©m s√£o relevantes, com seus valores influenciando a dire√ß√£o e magnitude do impacto na previs√£o de risco. [cite: 636]
        [cite_start]Em s√≠ntese, o modelo considera a dura√ß√£o e o valor do cr√©dito, o status da conta corrente, o prop√≥sito do empr√©stimo e a idade como os fatores mais determinantes para prever o risco de inadimpl√™ncia. [cite: 637]
        """
    )

    st.subheader("SHAP Waterfall Plot: Explica√ß√£o de Casos Individuais")
    st.write("Escolha um cliente para ver como suas caracter√≠sticas espec√≠ficas contribu√≠ram para a previs√£o de risco do modelo.")

    y_test_series = pd.Series(y_test, index=X_test_df.index)

    # Encontrar √≠ndices de um bom e um mau pagador para exemplo
    idx_good = y_test_series[y_test_series == 0].sample(1, random_state=42).index[0]
    idx_bad = y_test_series[y_test_series == 1].sample(1, random_state=42).index[0]

    selected_client_type = st.radio("Selecione o tipo de cliente para an√°lise:", ("Bom Pagador", "Mau Pagador"))

    if selected_client_type == "Bom Pagador":
        sample_index = idx_good
        sample_data = X_test_df.loc[[sample_index]]
        shap_value_sample = shap_values_class_1[X_test_df.index == sample_index][0]
        st.write(f"Analisando um **Cliente Bom Pagador** (√≠ndice original {sample_index}).")
    else:
        sample_index = idx_bad
        sample_data = X_test_df.loc[[sample_index]]
        shap_value_sample = shap_values_class_1[X_test_df.index == sample_index][0]
        st.write(f"Analisando um **Cliente Mau Pagador** (√≠ndice original {sample_index}).")

    expected_value = explainer.expected_value[1] if isinstance(explainer.expected_value, (list, np.ndarray)) else explainer.expected_value
    explanation_sample = shap.Explanation(
        values=shap_value_sample,
        base_values=expected_value,
        data=sample_data.values[0],
        feature_names=all_feature_names
    )

    fig_waterfall, ax_waterfall = plt.subplots(figsize=(10, 6))
    shap.plots.waterfall(explanation_sample, show=False)
    # Ajustar o t√≠tulo e labels
    plt.title(f"Waterfall Plot para {selected_client_type}")
    plt.xlabel("Valor SHAP")
    st.pyplot(fig_waterfall)
    plt.close(fig_waterfall)

    st.markdown(
        f"""
        **Interpreta√ß√£o do Waterfall Plot:**
        * O `f(x)` final no topo do gr√°fico representa a **probabilidade prevista pelo modelo** para este cliente espec√≠fico ser 'bad' (inadimplente).
        * O `E[f(X)]` (Expected Value) na base √© a **probabilidade m√©dia** de um cliente ser 'bad' na base de dados (aprox. {expected_value:.3f}).
        * As barras **vermelhas** indicam caracter√≠sticas que **aumentam** a probabilidade de ser 'bad'.
        * As barras **azuis** indicam caracter√≠sticas que **diminuem** a probabilidade de ser 'bad'.

        Voc√™ pode observar como cada caracter√≠stica individual (por exemplo, `purpose_other`, `duration`, `credit_amount`) empurra a previs√£o do valor base para o valor final previsto, tanto para um cliente bom quanto para um mau pagador.
        """
    )
    if selected_client_type == "Bom Pagador":
        st.markdown(
            """
            * [cite_start]**Exemplo 'Bom Pagador':** As caracter√≠sticas que mais contribu√≠ram para que este cliente fosse classificado como 'bom' foram geralmente os prop√≥sitos de cr√©dito espec√≠ficos (como 'outros' ou 'carro usado') e um valor de cr√©dito menor, superando os fatores de risco que o modelo identificou. [cite: 658]
            """
        )
    else:
        st.markdown(
            """
            * [cite_start]**Exemplo 'Mau Pagador':** Caracter√≠sticas como 'job_unskilled resident' (resid√™ncia n√£o qualificada) e 'purpose_domestic appliance' (prop√≥sito para eletrodom√©sticos) foram os maiores contribuintes para o risco, elevando a probabilidade de inadimpl√™ncia deste cliente. [cite: 672]
            """
        )

    # --- Tomada de Decis√£o e Aplica√ß√£o Gerencial ---
    st.header("5. Tomada de Decis√£o e Aplica√ß√£o Gerencial")
    [cite_start]st.markdown("Com base na an√°lise de explicabilidade usando SHAP values, o modelo Random Forest oferece informa√ß√µes cruciais para otimizar as estrat√©gias de concess√£o de cart√µes de cr√©dito, especialmente para jovens adultos e fam√≠lias de classe m√©dia. [cite: 673]")
    st.subheader("Recomenda√ß√µes para a √Årea de Cr√©dito:")
    st.markdown(
        """
        [cite_start]A institui√ß√£o deve implementar as seguintes diretrizes estrat√©gicas para equilibrar a expans√£o de clientes com a sustentabilidade financeira, utilizando a transpar√™ncia dos SHAP values: [cite: 675]

        * **Crit√©rios Aprimorados para Perfis de Alto Risco:** Clientes que solicitam cr√©ditos de longa dura√ß√£o e de valores elevados, e que apresentam um status de conta corrente menos favor√°vel, demonstram consistentemente um alto impacto SHAP para a classe "bad". [cite_start]Para esses perfis, sugere-se a aplica√ß√£o de crit√©rios de aprova√ß√£o mais rigorosos, como a redu√ß√£o dos limites de cr√©dito iniciais, a exig√™ncia de garantias adicionais ou a an√°lise aprofundada de sua capacidade de pagamento e hist√≥rico financeiro. [cite: 676, 677]
        * **Aten√ß√£o a Prop√≥sitos de Cr√©dito Espec√≠ficos e Perfil Ocupacional:** Os waterfall plots destacaram que prop√≥sitos de cr√©dito como "eletrodom√©sticos" (para mau pagador) e a profiss√£o de "residente n√£o qualificado" contribu√≠ram significativamente para o risco. [cite_start]Recomenda-se uma an√°lise mais detalhada para solicita√ß√µes com esses prop√≥sitos e para clientes com tal perfil ocupacional, podendo incluir a valida√ß√£o de estabilidade de renda e hist√≥rico de empregos. [cite: 678]
        * **Monitoramento Proativo para Mitiga√ß√£o de Risco:** Para clientes que se encaixam no p√∫blico-alvo (jovens adultos, classe m√©dia) mas que apresentam alguns fatores de risco moderados identificados pelo SHAP (ex: idade mais jovem), pode-se implementar um monitoramento proativo do comportamento de pagamento nos primeiros meses do contrato. [cite_start]Isso permitiria a oferta de suporte, educa√ß√£o financeira ou op√ß√µes de renegocia√ß√£o antes que a inadimpl√™ncia se consolide, visando mitigar o risco precocemente. [cite: 679, 680]
        [cite_start]Essas recomenda√ß√µes visam traduzir a intelig√™ncia do modelo de Machine Learning em a√ß√µes tang√≠veis para a √°rea de cr√©dito, permitindo uma tomada de decis√£o mais precisa e justificada para a gest√£o do risco e a expans√£o estrat√©gica da carteira de clientes. [cite: 681]
        """
    )

    # --- Modelos N√£o Supervisionados ---
    st.header("6. Modelos N√£o Supervisionados: Clusteriza√ß√£o e Outliers")
    st.write("Agora, exploraremos a segmenta√ß√£o de clientes e a detec√ß√£o de anomalias sem o uso da vari√°vel alvo.")

    # Usaremos X_processed (dados ap√≥s pr√©-processamento, antes do SMOTE) para clustering
    # Re-processar para garantir que X_processed seja o mesmo que no notebook original para esta se√ß√£o
    X_original_for_clustering = df.drop('class', axis=1) # Usar o DF original sem SMOTE
    X_processed_for_clustering = preprocessor.fit_transform(X_original_for_clustering)


    st.subheader("Clusteriza√ß√£o com KMeans")
    st.markdown("Utilizamos o M√©todo do Cotovelo e o Coeficiente de Silhueta para determinar o n√∫mero ideal de clusters.")

    # M√©todo do Cotovelo
    sse = []
    k_range = range(1, 11)
    for k in k_range:
        kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)
        kmeans.fit(X_processed_for_clustering)
        sse.append(kmeans.inertia_)

    fig_elbow, ax_elbow = plt.subplots(figsize=(8, 5))
    ax_elbow.plot(k_range, sse, marker='o')
    ax_elbow.set_title('M√©todo do Cotovelo para KMeans')
    ax_elbow.set_xlabel('N√∫mero de Clusters (K)')
    ax_elbow.set_ylabel('Soma dos Quadrados das Dist√¢ncias (SSE)')
    ax_elbow.grid(True)
    st.pyplot(fig_elbow)
    plt.close(fig_elbow)
    [cite_start]st.markdown("No gr√°fico do cotovelo, uma inflex√£o clara pode ser observada em $K=3$, sugerindo que a redu√ß√£o da SSE √© menos significativa ap√≥s este ponto. [cite: 761, 762]")

    # Coeficiente de Silhueta
    silhouette_scores = []
    for k in range(2, 11):
        kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)
        cluster_labels = kmeans.fit_predict(X_processed_for_clustering)
        score = silhouette_score(X_processed_for_clustering, cluster_labels)
        silhouette_scores.append(score)

    fig_sil, ax_sil = plt.subplots(figsize=(8, 5))
    ax_sil.plot(range(2, 11), silhouette_scores, marker='o')
    ax_sil.set_title('Coeficiente de Silhueta para KMeans')
    ax_sil.set_xlabel('N√∫mero de Clusters (K)')
    ax_sil.set_ylabel('Coeficiente de Silhueta')
    ax_sil.grid(True)
    st.pyplot(fig_sil)
    plt.close(fig_sil)
    [cite_start]st.markdown("Embora o pico do Coeficiente de Silhueta tenha sido observado em $K=2$, e $K=4$ tenha uma pontua√ß√£o ligeiramente superior a $K=3$, a escolha do K n√£o se baseia apenas na m√©trica isolada. [cite: 765]")

    n_clusters_chosen = st.slider("Escolha o n√∫mero de clusters (K) para KMeans:", min_value=2, max_value=5, value=3)
    [cite_start]st.markdown(f"**Escolha de K={n_clusters_chosen}:** A escolha de $K=3$ (valor padr√£o) √© justificada por ser o ponto de 'cotovelo' mais pronunciado e por buscar um equil√≠brio entre a redu√ß√£o da vari√¢ncia e a complexidade do modelo, fornecendo insights gerenciais mais ricos. [cite: 768]")

    kmeans_model = KMeans(n_clusters=n_clusters_chosen, random_state=42, n_init=10)
    kmeans_labels = kmeans_model.fit_predict(X_processed_for_clustering)
    df_with_clusters = df.copy()
    df_with_clusters['Cluster'] = kmeans_labels

    st.subheader(f"Perfis dos Clusters (KMeans com K={n_clusters_chosen})")
    st.write("An√°lise das m√©dias das features num√©ricas e das modas das features categ√≥ricas para cada cluster:")

    numeric_features_original = df.select_dtypes(include=['float64', 'int64']).columns.tolist()
    if 'class' in numeric_features_original:
        numeric_features_original.remove('class')

    st.markdown("#### M√©dia das Features Num√©ricas por Cluster:")
    st.dataframe(df_with_clusters.groupby('Cluster')[numeric_features_original].mean())

    st.markdown("#### Moda (Top 3) das Features Categ√≥ricas por Cluster:")
    for cluster_id in range(n_clusters_chosen):
        st.write(f"**Cluster {cluster_id}:**")
        cluster_df = df_with_clusters[df_with_clusters['Cluster'] == cluster_id]
        for col in categorical_features:
            top_categories = cluster_df[col].value_counts(normalize=True).head(3) * 100
            if not top_categories.empty:
                st.write(f"- {col}:")
                st.dataframe(top_categories.to_frame(name='Propor√ß√£o (%)'))


    st.markdown(
        """
        **Resumo dos Perfis (Baseado em K=3):**
        * **Cluster 0 (Maduros, Est√°veis, Risco Baixo):** Idade m√©dia mais alta (aprox. 45 anos), mais cr√©ditos existentes, maior tempo de resid√™ncia. Hist√≥rico de cr√©dito "critical/other existing credit", mas emprego de longa dura√ß√£o e casa pr√≥pria. [cite_start]Tendem a ser de menor risco. [cite: 1008, 1009, 1010, 1011]
        * **Cluster 1 (Cr√©dito Alto e Longo, Risco Alto):** Maior dura√ß√£o e valor de cr√©dito (aprox. 38.63 meses e 7831.21). Idade moderada. Hist√≥rico predominantemente "existing paid", mas maior propor√ß√£o de propriedade tipo "car" ou "no known property". [cite_start]Indicam perfis com certa maturidade, mas talvez com menor posse de bens imobili√°rios, buscando principalmente autom√≥veis. [cite: 1014, 1015, 1016, 1017, 1018, 1019]
        * **Cluster 2 (Jovens, Cr√©dito Baixo, Risco Moderado):** Idade m√©dia mais baixa (aprox. 29 anos), menor dura√ß√£o e valor de cr√©dito. Hist√≥rico predominantemente "existing paid". [cite_start]No entanto, demonstram alguma instabilidade de emprego e menor posse de telefone, o que pode ser um alerta. [cite: 1021, 1022, 1023, 1024, 1025, 1026]
        """
    )
    # Visualiza√ß√£o PCA dos Clientes Colorida por Cluster (KMeans)
    st.subheader("Visualiza√ß√£o PCA dos Clientes Colorida por Cluster (KMeans)")
    pca_kmeans = PCA(n_components=2, random_state=42)
    X_pca_kmeans = pca_kmeans.fit_transform(X_processed_for_clustering)
    df_pca_clusters = pd.DataFrame(data=X_pca_kmeans, columns=['Componente Principal 1', 'Componente Principal 2'])
    df_pca_clusters['Cluster'] = kmeans_labels
    df_pca_clusters['Original_Class'] = df['class']

    fig_pca_kmeans, ax_pca_kmeans = plt.subplots(figsize=(10, 7))
    sns.scatterplot(
        x='Componente Principal 1', y='Componente Principal 2',
        hue='Cluster', data=df_pca_clusters, palette='viridis',
        legend='full', alpha=0.7, s=50, ax=ax_pca_kmeans
    )
    ax_pca_kmeans.set_title(f'Visualiza√ß√£o PCA dos Clientes Colorida por Cluster (KMeans com K={n_clusters_chosen})')
    ax_pca_kmeans.set_xlabel(f'Componente Principal 1 ({pca_kmeans.explained_variance_ratio_[0]*100:.2f}% vari√¢ncia explicada)')
    ax_pca_kmeans.set_ylabel(f'Componente Principal 2 ({pca_kmeans.explained_variance_ratio_[1]*100:.2f}% vari√¢ncia explicada)')
    ax_pca_kmeans.grid(True)
    st.pyplot(fig_pca_kmeans)
    plt.close(fig_pca_kmeans)

    st.markdown("---")

    st.subheader("Detec√ß√£o de Outliers com DBSCAN")
    [cite_start]st.markdown("O DBSCAN √© utilizado para identificar clientes at√≠picos na base de dados. [cite: 1071]")

    # Gr√°fico de k-dist√¢ncia para determinar Epsilon
    k_neighbors_dbscan = st.slider("Escolha k-vizinhos para o gr√°fico de k-dist√¢ncia:", min_value=10, max_value=50, value=30)
    nbrs = NearestNeighbors(n_neighbors=k_neighbors_dbscan).fit(X_processed_for_clustering)
    distances, indices = nbrs.kneighbors(X_processed_for_clustering)
    distances_kth_neighbor = np.sort(distances[:, k_neighbors_dbscan-1], axis=0)

    fig_kdist, ax_kdist = plt.subplots(figsize=(10, 6))
    ax_kdist.plot(distances_kth_neighbor)
    ax_kdist.set_title("Gr√°fico de k-dist√¢ncia para Determina√ß√£o de Epsilon (DBSCAN)")
    ax_kdist.set_xlabel("√çndice das Amostras (ordenado por dist√¢ncia)")
    ax_kdist.set_ylabel(f"Dist√¢ncia para o {k_neighbors_dbscan}-√©simo Vizinho Mais Pr√≥ximo")
    ax_kdist.grid(True)
    st.pyplot(fig_kdist)
    plt.close(fig_kdist)
    st.markdown("Observe o 'cotovelo' (maior inclina√ß√£o) no gr√°fico acima. Este ponto sugere o valor apropriado para 'eps'.")
    st.markdown("O valor de `eps` deve ser o valor no eixo Y onde a curva muda abruptamente.")


    st.markdown("#### Par√¢metros do DBSCAN (Ajust√°veis)")
    eps_chosen_optimized = st.slider("Valor de Epsilon (eps):", min_value=0.5, max_value=10.0, value=3.7, step=0.1)
    min_samples_chosen_optimized = st.slider("M√≠nimo de Amostras (min_samples):", min_value=5, max_value=100, value=30)
    [cite_start]st.markdown(f"Par√¢metros otimizados: `eps={eps_chosen_optimized}` e `min_samples={min_samples_chosen_optimized}`. [cite: 1075]")

    dbscan_optimized = DBSCAN(eps=eps_chosen_optimized, min_samples=min_samples_chosen_optimized)
    dbscan_labels_optimized = dbscan_optimized.fit_predict(X_processed_for_clustering)

    outliers_count_optimized = np.sum(dbscan_labels_optimized == -1)
    clusters_formed = len(np.unique(dbscan_labels_optimized)) - (1 if -1 in dbscan_labels_optimized else 0)

    st.write(f"**Resultados da Detec√ß√£o de Outliers com DBSCAN:**")
    st.write(f"- Total de amostras: {len(dbscan_labels_optimized)}")
    st.write(f"- N√∫mero de Clusters formados: {clusters_formed}")
    st.write(f"- N√∫mero de Outliers detectados: {outliers_count_optimized}")
    st.write(f"- Propor√ß√£o de Outliers: {(outliers_count_optimized/len(dbscan_labels_optimized)*100):.2f}%")
    [cite_start]st.markdown("Esses resultados indicam que o DBSCAN conseguiu formar um √∫nico e grande cluster que abrange a maior parte dos dados densos, enquanto uma propor√ß√£o das amostras foi classificada como ru√≠do (outliers). [cite: 1174]")

    core_samples_mask = np.zeros_like(dbscan_optimized.labels_, dtype=bool)
    if hasattr(dbscan_optimized, 'core_sample_indices_'):
        core_samples_mask[dbscan_optimized.core_sample_indices_] = True
    labels_raw = dbscan_optimized.labels_

    core_points_count = np.sum(core_samples_mask)
    outlier_points_count = np.sum(labels_raw == -1)
    border_points_count = len(labels_raw) - core_points_count - outlier_points_count

    st.write(f"- Total de Pontos de N√∫cleo (Core Points): {core_points_count}")
    st.write(f"- Total de Pontos de Borda (Border Points): {border_points_count}")
    st.write(f"- Total de Outliers (Noise Points): {outlier_points_count}")
    [cite_start]st.markdown("Essa distribui√ß√£o mostra uma estrutura clara: uma parte substancial dos dados forma um n√∫cleo denso, com um n√∫mero significativo de pontos de borda estendendo o cluster, e um grupo menor e identific√°vel de verdadeiros outliers. [cite: 1179]")


    # Rela√ß√£o entre Outliers e Inadimpl√™ncia
    df_with_dbscan_labels_optimized = df.copy()
    df_with_dbscan_labels_optimized['DBSCAN_Label'] = dbscan_labels_optimized
    df_outliers_optimized = df_with_dbscan_labels_optimized[df_with_dbscan_labels_optimized['DBSCAN_Label'] == -1]

    if not df_outliers_optimized.empty:
        bad_outliers_count_optimized = df_outliers_optimized[df_outliers_optimized['class'] == 1].shape[0]
        total_outliers_count_optimized = df_outliers_optimized.shape[0]
        if total_outliers_count_optimized > 0:
            proportion_bad_in_outliers_optimized = (bad_outliers_count_optimized / total_outliers_count_optimized) * 100
            overall_bad_proportion = (df['class'] == 1).sum() / len(df) * 100
            [cite_start]st.markdown(f"**Propor√ß√£o de clientes 'bad' entre os Outliers: {proportion_bad_in_outliers_optimized:.2f}%** [cite: 1154]")
            [cite_start]st.markdown(f"**Propor√ß√£o geral de clientes 'bad' na base de dados: {overall_bad_proportion:.2f}%** [cite: 1154]")
            [cite_start]st.markdown("Ao comparar essas propor√ß√µes, √© evidente que os outliers identificados pelo DBSCAN possuem uma propor√ß√£o significativamente maior de clientes 'bad' (44.81%) do que a m√©dia da base de dados (30.00%). [cite: 1185, 1183, 1184]")
            [cite_start]st.markdown("Isso indica uma forte rela√ß√£o: os perfis at√≠picos detectados pelo DBSCAN s√£o consideravelmente mais propensos a serem 'maus pagadores'. Esse grupo de outliers merece aten√ß√£o especial para avalia√ß√£o de risco, pois suas caracter√≠sticas incomuns est√£o ligadas a uma maior probabilidade de inadimpl√™ncia. [cite: 1186, 1187]")

            st.markdown("#### Perfil dos Outliers (DBSCAN):")
            st.write("Analisando as caracter√≠sticas dos clientes classificados como outliers para entender o que os torna at√≠picos e mais arriscados:")
            outlier_numeric_means = df_outliers_optimized[numeric_features_original].mean()
            st.markdown("**M√©dia das Features Num√©ricas para Outliers:**")
            st.dataframe(outlier_numeric_means.to_frame(name='M√©dia'))

            st.markdown("**Moda (Top 3) das Features Categ√≥ricas para Outliers:**")
            for col in categorical_features:
                top_categories_outlier = df_outliers_optimized[col].value_counts(normalize=True).head(3) * 100
                if not top_categories_outlier.empty:
                    st.write(f"- {col}:")
                    st.dataframe(top_categories_outlier.to_frame(name='Propor√ß√£o (%)'))

            st.markdown(
                """
                **Interpreta√ß√£o do Perfil dos Outliers:**
                * O perfil exato dos outliers depender√° das suas caracter√≠sticas espec√≠ficas. No geral, eles s√£o indiv√≠duos que n√£o se encaixam nos padr√µes de densidade dos clusters principais. Isso pode significar, por exemplo, que possuem uma combina√ß√£o rara de alto valor de cr√©dito com baixa idade, ou um hist√≥rico de cr√©dito muito at√≠pico, ou uma dura√ß√£o de empr√©stimo excepcionalmente longa para um dado prop√≥sito.
                * A alta propor√ß√£o de 'bad' entre os outliers (44.81%) confirma que essas caracter√≠sticas at√≠picas est√£o associadas a um risco significativamente maior de inadimpl√™ncia. Isso refor√ßa a necessidade de uma an√°lise manual ou crit√©rios de aprova√ß√£o extremamente rigorosos para esses perfis.
                """
            )

        else:
            st.warning("N√£o foram detectados outliers pelo DBSCAN com os par√¢metros otimizados para an√°lise.")
    else:
        st.warning("N√£o foram detectados outliers pelo DBSCAN com os par√¢metros otimizados para an√°lise.")

    # Visualiza√ß√£o PCA dos Clientes Colorida por R√≥tulos DBSCAN
    st.subheader("Visualiza√ß√£o PCA dos Clientes Colorida por R√≥tulos DBSCAN")
    pca_dbscan_plot = PCA(n_components=2, random_state=42)
    X_pca_dbscan_plot = pca_dbscan_plot.fit_transform(X_processed_for_clustering)
    df_pca_dbscan_plot = pd.DataFrame(data=X_pca_dbscan_plot, columns=['Componente Principal 1', 'Componente Principal 2'])
    df_pca_dbscan_plot['DBSCAN_Label'] = dbscan_labels_optimized
    df_pca_dbscan_plot['Original_Class'] = df['class']

    fig_pca_dbscan, ax_pca_dbscan = plt.subplots(figsize=(12, 8))
    sns.scatterplot(
        x='Componente Principal 1', y='Componente Principal 2',
        hue='DBSCAN_Label', data=df_pca_dbscan_plot, palette='Spectral',
        legend='full', alpha=0.7, s=50, ax=ax_pca_dbscan
    )
    ax_pca_dbscan.set_title(f'Visualiza√ß√£o PCA dos Clientes Colorida por R√≥tulos DBSCAN (eps={eps_chosen_optimized}, min_samples={min_samples_chosen_optimized})')
    ax_pca_dbscan.set_xlabel(f'Componente Principal 1 ({pca_dbscan_plot.explained_variance_ratio_[0]*100:.2f}% vari√¢ncia explicada)')
    ax_dbscan.set_ylabel(f'Componente Principal 2 ({pca_dbscan_plot.explained_variance_ratio_[1]*100:.2f}% vari√¢ncia explicada)')
    ax_pca_dbscan.grid(True)
    st.pyplot(fig_pca_dbscan)
    plt.close(fig_pca_dbscan)
    [cite_start]st.markdown("A visualiza√ß√£o mostra claramente um grande cluster central (pontos roxos) que abrange a maioria dos dados, e os outliers (pontos vermelhos) localizados predominantemente na periferia ou em regi√µes mais esparsas, confirmando sua natureza de pontos menos densos ou isolados. [cite: 1191, 1192]")

    st.markdown("---")

    st.subheader("An√°lise Cruzada: Risco de Inadimpl√™ncia por Cluster (KMeans)")
    st.write("Verificando a distribui√ß√£o da vari√°vel 'class' (inadimpl√™ncia) nos clusters do KMeans para identificar agrupamentos mais arriscados.")

    class_distribution_by_cluster = df_with_clusters.groupby('Cluster')['class'].value_counts(normalize=True).unstack(fill_value=0) * 100
    if 0 in class_distribution_by_cluster.columns and 1 in class_distribution_by_cluster.columns:
        class_distribution_by_cluster.rename(columns={0: 'good', 1: 'bad'}, inplace=True)

    fig_risk_by_cluster, ax_risk_by_cluster = plt.subplots(figsize=(8, 5))
    bad_col_for_plot = 'bad' if 'bad' in class_distribution_by_cluster.columns else 1 # Garantir nome correto
    sns.barplot(x=class_distribution_by_cluster.index, y=class_distribution_by_cluster[bad_col_for_plot], palette='coolwarm', ax=ax_risk_by_cluster)
    ax_risk_by_cluster.set_title(f'Propor√ß√£o de Inadimplentes (Classe "bad") por Cluster (KMeans com K={n_clusters_chosen})')
    ax_risk_by_cluster.set_xlabel('Cluster')
    ax_risk_by_cluster.set_ylabel('Propor√ß√£o de Inadimplentes (%)')
    ax_risk_by_cluster.set_ylim(0, 50) # Ajusta o limite Y para melhor visualiza√ß√£o
    ax_risk_by_cluster.grid(axis='y')
    st.pyplot(fig_risk_by_cluster)
    plt.close(fig_risk_by_cluster)

    st.markdown(
        """
        [cite_start]A an√°lise cruzada da clusteriza√ß√£o KMeans com a vari√°vel-alvo `class` √© fundamental para identificar segmentos de clientes com perfis de risco distintos. [cite: 1223]

        * **Cluster 0 (Baixo Risco):** Apresenta a menor propor√ß√£o de clientes 'bad', com aproximadamente 21% de inadimplentes. [cite_start]Representa o segmento de menor risco e pode ser foco de campanhas de aquisi√ß√£o. [cite: 1226, 1228, 1229]
        * **Cluster 1 (Alto Risco):** Se destaca como o grupo de maior risco, concentrando a mais alta taxa de clientes 'bad', com aproximadamente 46-47% de inadimplentes. [cite_start]Exige pol√≠ticas de cr√©dito rigorosas. [cite: 1231, 1232, 1233]
        * **Cluster 2 (Risco Moderado):** Apresenta uma propor√ß√£o de inadimplentes de aproximadamente 30%, alinhada com a m√©dia geral da base de dados. [cite_start]Representa um risco intermedi√°rio, podendo ter estrat√©gias de acompanhamento pr√≥ximo. [cite: 1235, 1236]

        A segmenta√ß√£o dos clientes por KMeans, ao revelar a concentra√ß√£o de risco em diferentes perfis, oferece √† institui√ß√£o financeira uma ferramenta estrat√©gica poderosa. [cite_start]Essa an√°lise permite uma tomada de decis√£o mais informada e direcionada, otimizando a aloca√ß√£o de recursos, a gest√£o de riscos e o desenvolvimento de ofertas de produtos personalizadas para cada segmento de cliente. [cite: 1238, 1239]
        """
    )
else:
    st.warning("Por favor, fa√ßa o upload do arquivo 'credit_customers.csv' ou verifique o caminho do arquivo de exemplo.")
